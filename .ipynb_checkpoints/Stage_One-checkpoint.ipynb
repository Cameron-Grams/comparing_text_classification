{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "434375cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52dba068",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ea8b1eac0c76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutilities\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mut\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mModels\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Models'"
     ]
    }
   ],
   "source": [
    "import utilities as ut\n",
    "import Models as M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb0d225",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = './data/WikiLarge_Train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d145e3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = ut.produce_dataframe(training_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a884f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d261f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for development purposes\n",
    "df_x = df.sample(frac=1, random_state=42)\n",
    "df_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6278e7d",
   "metadata": {},
   "source": [
    "## Add concreteness and the percent recognized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9050eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concreteness dataframe for reference\n",
    "conc_df = pd.read_csv('./data/Concreteness_ratings_Brysbaert_et_al_BRM.csv', \n",
    "                 skiprows=0, \n",
    "                 error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adc57df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_plus_conc = ut.derive_concrete_score(df_x, conc_df)\n",
    "df_plus_conc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0525de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d = df_plus_conc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb524ff",
   "metadata": {},
   "source": [
    "## Add the Dale-Chall scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4337d396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dale-chall list in as an index\n",
    "dale_chall_list = ut.create_dale_chall_list('./data/dale_chall.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eca465",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_d['dale_chall_count'] = df_d.apply(lambda x: ut.dale_chall_check_each_token(x['tokens'], dale_chall_list), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7579ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900dd865",
   "metadata": {},
   "source": [
    "## Add Age of Acquisition and 2nd recognition measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e59f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_age = df_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bd0e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "aoa_df = pd.read_csv('./data/AoA_51715_words.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f00275",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_main = ut.derive_age_of_acquisition(df_age, aoa_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249e15af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8c1e98",
   "metadata": {},
   "source": [
    "## Add Flesch-Kincaid Readability Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90decd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = ut.produce_flesch_kincaid(df_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7b196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a715ce9",
   "metadata": {},
   "source": [
    "# Create the Derived Features Dataframe to train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ffe118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5252b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "derived_features = ['sentence_length',\n",
    " 'concreteness',\n",
    " 'recognized',\n",
    " 'dale_chall_count',\n",
    " 'aoa',\n",
    " 'perc_known',\n",
    " 'fk_ease',\n",
    " 'fk_grade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbcca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[derived_features]\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77861368",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a3895d",
   "metadata": {},
   "source": [
    "# Compare the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70c3c2b",
   "metadata": {},
   "source": [
    "## Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05bfab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "d_score, score, graphic = M.logistic_regrssions_classifier_assessment(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64393fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dummy classifer score: {np.round(d_score, 4)}\")\n",
    "print(f\"F1 Score using a Logistic Regression Classifier: {np.round(score, 4)}\")\n",
    "graphic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b2a114",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dummy classifer score: {np.round(d_score, 4)}\")\n",
    "print(f\"F1 Score using a Logistic Regression Classifier: {np.round(score, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071af4b7",
   "metadata": {},
   "source": [
    "## SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e0bf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# d_score, score, graphic = M.SVM_classifier_assessment(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e7c1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Dummy classifer score: {np.rounc(d_score, 4)}\")\n",
    "# print(f\"F1 Score using a SVM Classifier: {np.round(score, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f278b50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphic()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e7ba5c",
   "metadata": {},
   "source": [
    "## RandomForest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268d3083",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "d_score, score, comparison, graphic = M.RandomForest_classifier_assessment(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b28722e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dummy classifer score: {np.round(d_score, 4)}\")\n",
    "print(f\"F1 Score using a RandomForest Classifier: {np.round(score, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9f9cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dummy classifer score: {np.round(d_score, 4)}\")\n",
    "print(f\"F1 Score using a RandomForest Classifier: {np.round(score, 4)}\")\n",
    "graphic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ea4c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e849fa8f",
   "metadata": {},
   "source": [
    "## NaiveBayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adaf444",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "d_score, score, graphic = M.NaiveBayes_classifier_assessment(X, y, priors=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8027ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dummy classifer score: {np.round(d_score, 4)}\")\n",
    "print(f\"F1 Score using a NaiveBayes Classifier: {np.round(score, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0388352a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dummy classifer score: {np.round(d_score, 4)}\")\n",
    "print(f\"F1 Score using a NaiveBayes Classifier: {np.round(score, 4)}\")\n",
    "graphic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c02224",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
